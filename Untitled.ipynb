{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stock-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "homeless-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('projects_links.json') as file:\n",
    "#   linkinghost = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greatest-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('project_data.json') as file:\n",
    "  host = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seeing-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostList = []\n",
    "chostList = []\n",
    "linkinghostList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPID(val):\n",
    "    valhost = val.split(\"?\")\n",
    "    idhost = valhost[1].split(\"=\")\n",
    "    pureid = idhost[1]\n",
    "    if pureid is None:\n",
    "        return \"\"\n",
    "    return pureid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infrared-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSpecialChar(val):\n",
    "    pureval = val.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    return pureval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southwest-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSpecialCharAndSpace(val):\n",
    "    pureval = val.replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\")\n",
    "    return pureval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "competitive-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(passval, name):\n",
    "    completename = os.path.join('./scrapeddata/', name)\n",
    "    try:\n",
    "        with open(completename, 'w') as outfile:\n",
    "            json.dump(passval, outfile, ensure_ascii=False, indent=4)\n",
    "        print(completename)\n",
    "    except:\n",
    "        print(\"\\n NO AREACODE AND LOCATION OBJECT VALUES!\")\n",
    "        print(completename)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "steady-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting project_html to json objects\n",
    "def getPHTML(val, idval):\n",
    "    summary = {}    \n",
    "# #     inserting Pid value to summary\n",
    "    summary['Pid']=idval\n",
    "    \n",
    "# #     processing project summary information\n",
    "    psumHeads = []\n",
    "    psumVals = []\n",
    "    psumdata = {}\n",
    "    psum_soup = BeautifulSoup(val, 'html.parser')\n",
    "    psumhostdata = psum_soup.find('div', attrs={'id': 'dsummary'}).find('table')\n",
    "    psumheadvals = psumhostdata.findAll('th')\n",
    "    psumvals = psumhostdata.findAll('td')\n",
    "    for ps in psumheadvals:\n",
    "        psumHeads.append(ps.text.replace(\" \", \"\"))\n",
    "    for count, psval in enumerate(psumvals):\n",
    "        psumdata[psumHeads[count]]=cleanSpecialChar(psval.text)              \n",
    "    psumVals.append(psumdata)\n",
    "    summary['ProjectSummary']=psumVals\n",
    "    \n",
    "        \n",
    "# #     processing budget part\n",
    "    budgetHeads = []\n",
    "    budgetVals = []\n",
    "    budgetdata = {}\n",
    "    soup = BeautifulSoup(val, 'html.parser')\n",
    "    hostdata = soup.find('div', attrs={'id': 'p_budget'}).find('div').find('table')\n",
    "    headvals = hostdata.findAll('th')\n",
    "    vals = hostdata.findAll('td')\n",
    "    for h in headvals:\n",
    "        budgetHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for count, d in enumerate(vals):\n",
    "        if count < len(budgetHeads):\n",
    "            budgetdata[budgetHeads[count]]=d.text\n",
    "    budgetVals.append(budgetdata)\n",
    "    summary['Budget']=budgetVals\n",
    "    \n",
    "    \n",
    "# #     processing beneficiary part\n",
    "    beneficiaryHeads = ['No', 'Village', 'BeneficiaryTotal', 'BeneficiaryWomen', 'Households']\n",
    "    raw_head = []\n",
    "    beneficiaryVals = []\n",
    "    beneficiarydata = {}\n",
    "    benehost = {}\n",
    "    bene_soup = BeautifulSoup(val, 'html.parser')\n",
    "    hostbenedata = bene_soup.find('div', attrs={'id': 'p_beneficary'}).find('div').find('table')\n",
    "    beneheadvals = hostbenedata.findAll('th')\n",
    "    benevals = hostbenedata.findAll('td')\n",
    "    for h in beneheadvals:\n",
    "        raw_head.append(h.text.replace(\" \", \"\"))\n",
    "    for bene_count, bene_d in enumerate(benevals):\n",
    "        if bene_count < 5:\n",
    "            beneficiarydata[beneficiaryHeads[bene_count]]=cleanSpecialCharAndSpace(bene_d.text)\n",
    "    beneficiaryVals.append(beneficiarydata)\n",
    "    summary['Beneficiary']=beneficiaryVals\n",
    "\n",
    "\n",
    "# # processing output part\n",
    "    outputHeads = []\n",
    "    outputVals = []\n",
    "    outputdata = {}\n",
    "    outputhost = {}\n",
    "    output_soup = BeautifulSoup(val, 'html.parser')\n",
    "    outputhostdata = output_soup.find('div', attrs={'id': 'p_outputs'}).find('div').find('table')\n",
    "    outputheadvals = outputhostdata.findAll('th')\n",
    "    outputvals = outputhostdata.findAll('td')\n",
    "    for h in outputheadvals:\n",
    "        outputHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for out_count, out_val in enumerate(outputvals):\n",
    "        if out_count < 6:\n",
    "            outputdata[outputHeads[out_count]]=cleanSpecialChar(out_val.text)\n",
    "    outputVals.append(outputdata)\n",
    "    summary['Outputs_Estimated_Cost']=outputVals\n",
    "\n",
    "\n",
    "# # processing technical part\n",
    "    tech_soup = BeautifulSoup(val, 'html.parser')\n",
    "    tech_main_data = tech_soup.find('div', attrs={'id': 'p_clearance'})\n",
    "    techform = {}\n",
    "    \n",
    "#   Evironmental safeguard clearance \n",
    "    envHeads = []\n",
    "    envVals = []\n",
    "    envdata = {}\n",
    "    envhost = {}\n",
    "    tech_env_data = tech_main_data.find('div', attrs={'id' : 'divEA'})\n",
    "    envheadvals = tech_env_data.findAll('th')\n",
    "    envvals = tech_env_data.findAll('td')\n",
    "    for e in envheadvals:\n",
    "        envHeads.append(e.text.replace(\" \", \"\"))\n",
    "    for e_count, e_val in enumerate(envvals):\n",
    "        if e_count < len(envHeads):\n",
    "            envdata[envHeads[e_count]]=cleanSpecialChar(e_val.text)\n",
    "    envVals.append(envdata)\n",
    "    techform['Evironmental_Safeguard']=envVals\n",
    "    \n",
    "#   Land safeguard clearance\n",
    "    landHeads = []\n",
    "    landVals = []\n",
    "    landdata = {}\n",
    "    landhost = {}\n",
    "    tech_land_data = tech_main_data.find('div', attrs={'id' : 'divLAR'})\n",
    "    landheadvals = tech_land_data.findAll('th')\n",
    "    landvals = tech_land_data.findAll('td')\n",
    "    for l in landheadvals:\n",
    "        landHeads.append(l.text.replace(\" \", \"\"))\n",
    "    for l_count, l_val in enumerate(landvals):\n",
    "        landdata[landHeads[l_count]]=cleanSpecialChar(l_val.text)\n",
    "    landVals.append(landdata)\n",
    "    techform['Land_Safeguard']=landVals\n",
    "    \n",
    "#   Highland people safeguard clearance   \n",
    "    highHeads = []\n",
    "    highVals = []\n",
    "    highdata = {}\n",
    "    highhost = {}\n",
    "    tech_highland_data = tech_main_data.find('div', attrs={'id' : 'divHP'})\n",
    "    highheadvals = tech_highland_data.findAll('th')\n",
    "    highvals = tech_highland_data.findAll('td')\n",
    "    for h in highheadvals:\n",
    "        highHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for h_count, h_val in enumerate(highvals):\n",
    "        if h_count < len(highHeads):\n",
    "            highdata[highHeads[h_count]]=cleanSpecialChar(h_val.text)\n",
    "    highVals.append(highdata)\n",
    "    techform['Highland_Safeguard']=highVals\n",
    "    \n",
    "#   Sign-Off\n",
    "    signHeads = []\n",
    "    signVals = []\n",
    "    signdata = {}\n",
    "    signhost = {}\n",
    "    tech_signoff_data = tech_main_data.find('div', attrs={'id' : 'pSignOff'})\n",
    "    signheadvals = tech_signoff_data.findAll('th')\n",
    "    signvals = tech_signoff_data.findAll('td')\n",
    "    for s in signheadvals:\n",
    "        signHeads.append(s.text.replace(\" \", \"\"))\n",
    "    for s_count, s_val in enumerate(signvals):\n",
    "        signdata[signHeads[s_count]]=cleanSpecialChar(s_val.text)\n",
    "    signVals.append(signdata)\n",
    "    techform['Signoff']=signVals\n",
    "    summary['Technical_Clearance']=techform\n",
    "    \n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intended-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting contract_html to json objects\n",
    "def getCHTML(val, idval):\n",
    "    contract_summary = {}\n",
    "    \n",
    "# #     inserting id value to contract summary\n",
    "    contract_summary['Id']=idval\n",
    "    \n",
    "# #     processing contract summary information\n",
    "    csumHeads = []\n",
    "    csumVals = []\n",
    "    csumdata = {}\n",
    "    csum_soup = BeautifulSoup(val, 'html.parser')\n",
    "    csumhostdata = csum_soup.find('div', attrs={'id': 'dhead'}).find('table')\n",
    "    csumheadvals = csumhostdata.findAll('th')\n",
    "    csumvals = csumhostdata.findAll('td')\n",
    "    for cs in csumheadvals:\n",
    "        csumHeads.append(cs.text.replace(\" \", \"\"))\n",
    "    bothcsumhostdata = csum_soup.find('div', attrs={'id': 'dhead'}).findAll('table')\n",
    "    if len(bothcsumhostdata) == 2:\n",
    "        csumheadonevals = bothcsumhostdata[1].findAll('th')\n",
    "        csumonevals = bothcsumhostdata[1].findAll('td')\n",
    "        for cs in csumheadonevals:\n",
    "            csumHeads.append(cs.text.replace(\" \", \"\"))\n",
    "        for csov in csumonevals:\n",
    "            csumvals.append(csov)\n",
    "    for count, csval in enumerate(csumvals):\n",
    "        csumdata[csumHeads[count]]=cleanSpecialChar(csval.text)\n",
    "    csumVals.append(csumdata)\n",
    "    contract_summary['ContractSummary']=csumVals\n",
    "\n",
    "    \n",
    "# #     processing budget part\n",
    "    budgetHeads = []\n",
    "    budgetVals = []\n",
    "    rawVals = []\n",
    "    rawbvals = []\n",
    "    budgetdata = {}\n",
    "    soup = BeautifulSoup(val, 'html.parser')\n",
    "    hostdata = soup.find('div', attrs={'id': 'dbudget'}).find('table')\n",
    "    budgetheadvals = hostdata.findAll('th')\n",
    "    budgetvals = hostdata.findAll('td')\n",
    "    for h in budgetheadvals:\n",
    "        budgetHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for budget_count, budget_val in enumerate(budgetvals):\n",
    "        rawbvals.append(cleanSpecialChar(budget_val.text))\n",
    "    for bvc in range(int(len(rawbvals)/2)):\n",
    "        bvcval = bvc+1\n",
    "        btdval = soup.find('div', attrs={'id': 'dbudget'}).find('table').find('tr', attrs={'id': 'row_cb_'+str(bvcval)+''}).findAll('td')\n",
    "        bvform = {}\n",
    "        for c, td in enumerate(btdval):\n",
    "            bvform[budgetHeads[c]]=td.text\n",
    "        budgetVals.append(bvform)\n",
    "    contract_summary['Budget']=budgetVals\n",
    "    \n",
    "    \n",
    "# #     processing bidding part\n",
    "    biddingHeads = [\"No\"]\n",
    "    raw_head = []\n",
    "    bidVals = []\n",
    "    biddata = {}\n",
    "    bidhost = {}\n",
    "    rawdatalist = []\n",
    "    bid_soup = BeautifulSoup(val, 'html.parser')\n",
    "    try:\n",
    "        hostbiddata = bid_soup.find('div', attrs={'id': 'dlbid'}).find('table')\n",
    "        bidheadvals = hostbiddata.findAll('th')\n",
    "        bidvals = hostbiddata.findAll('td')\n",
    "        bidtrs = hostbiddata.find('tbody').findAll('tr')\n",
    "        for h in bidheadvals:\n",
    "            biddingHeads.append(h.text.replace(\" \", \"\"))\n",
    "        del biddingHeads[1]\n",
    "        del biddingHeads[-1]\n",
    "        for b_count, b_tr in enumerate(bidtrs):\n",
    "            numval = b_count+1\n",
    "            bidtdval = hostbiddata.find('tbody').find('tr', attrs={'id': 'tbbs_'+str(numval)+''}).findAll('td')\n",
    "            bidform = {}\n",
    "            for c, td in enumerate(bidtdval):\n",
    "                bidform[biddingHeads[c]]=cleanSpecialCharAndSpace(td.text)\n",
    "            bidVals.append(bidform)\n",
    "    except:\n",
    "        bidVals.append([])\n",
    "    contract_summary['Bidding']=bidVals\n",
    "\n",
    "\n",
    "\n",
    "# # processing output part\n",
    "    outputHeads = []\n",
    "    outputVals = []\n",
    "    outputdata = {}\n",
    "    outputhost = {}\n",
    "    output_soup = BeautifulSoup(val, 'html.parser')\n",
    "    outputhostdata = output_soup.find('div', attrs={'id': 'c_outputs'}).find('div').find('table')\n",
    "    outputheadvals = outputhostdata.findAll('th')\n",
    "    outputvals = outputhostdata.findAll('td')\n",
    "    for h in outputheadvals:\n",
    "        outputHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for out_count, out_val in enumerate(outputvals):\n",
    "        if out_count < 9:\n",
    "            outputdata[outputHeads[out_count]]=cleanSpecialChar(out_val.text)\n",
    "        else:\n",
    "            outputdata[outputHeads[-2]]=outputHeads[-1]\n",
    "        outputdata[outputHeads[-2]]=outputHeads[-1]\n",
    "    outputVals.append(outputdata)\n",
    "    contract_summary['Outputs_Actual_Cost']=outputVals\n",
    "    \n",
    "    \n",
    "# # processing progress part\n",
    "    pureprogressVals = []\n",
    "# # processing progress/work part\n",
    "    workHeads = []\n",
    "    workVals = []\n",
    "    rawwvals = []\n",
    "    workdata = {}\n",
    "    workhost = {}\n",
    "    work_soup = BeautifulSoup(val, 'html.parser')\n",
    "    workhostdata = work_soup.find('div', attrs={'id': 'd_actual_work'}).find('table')\n",
    "    workheadvals = workhostdata.findAll('th')\n",
    "    workvals = workhostdata.findAll('td')\n",
    "    for h in workheadvals:\n",
    "        workHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for work_count, work_val in enumerate(workvals):\n",
    "        rawwvals.append(cleanSpecialChar(work_val.text))\n",
    "    workdata[workHeads[0]]=cleanSpecialChar(rawwvals[0])\n",
    "    workdata[workHeads[1]]=cleanSpecialChar(rawwvals[1])\n",
    "    try:\n",
    "        workdata['ActualworkstartdateDescription']=cleanSpecialChar(rawwvals[2])\n",
    "        workdata['ActualworkcompletiondateDescription']=cleanSpecialChar(rawwvals[3])\n",
    "    except:\n",
    "        print(\"\")\n",
    "    pureprogressVals.append(workdata)\n",
    "\n",
    "\n",
    "# # processing progress/progress part\n",
    "    progressHeads = []\n",
    "    progressVals = []\n",
    "    rawpvals = []\n",
    "    progressdata = {}\n",
    "    progresshost = {}\n",
    "    progress_soup = BeautifulSoup(val, 'html.parser')\n",
    "    progresshostdata = progress_soup.find('div', attrs={'id': 'dlprogress'}).find('table')\n",
    "    progressheadvals = progresshostdata.findAll('th')\n",
    "    progressvals = progresshostdata.findAll('td')\n",
    "    for h in progressheadvals:\n",
    "        progressHeads.append(h.text.replace(\" \", \"\"))\n",
    "    for progress_count, progress_val in enumerate(progressvals):\n",
    "        rawpvals.append(cleanSpecialChar(progress_val.text))\n",
    "    for pvc in range(int(len(rawpvals)/4)):\n",
    "        pvcval = pvc+1\n",
    "        ptdval = progress_soup.find('div', attrs={'id': 'dlprogress'}).find('table').find('tr', attrs={'id': 'row_cp_'+str(pvcval)+''}).findAll('td')\n",
    "        pvform = {}\n",
    "        for c, td in enumerate(ptdval):\n",
    "            pvform[progressHeads[c]]=td.text\n",
    "        progressVals.append(pvform)\n",
    "    pureprogressVals.append(progressVals)\n",
    "    contract_summary['progress']=pureprogressVals\n",
    "    \n",
    "        \n",
    "# #     processing payment part\n",
    "    paymentHeads = []\n",
    "    raw_head = []\n",
    "    paymentVals = []\n",
    "    paymentdata = {}\n",
    "    payhost = {}\n",
    "    pform = {}\n",
    "    pay_soup = BeautifulSoup(val, 'html.parser')\n",
    "    hostpaydata = pay_soup.find('div', attrs={'id': 'c_payment'}).find('div').find('table')\n",
    "    payheadvals = hostpaydata.findAll('th')\n",
    "    payvals = hostpaydata.find('tr').findAll('tr')\n",
    "    for h in payheadvals:\n",
    "        paymentHeads.append(h.text.replace(\" \", \"\"))\n",
    "    payvals.pop()\n",
    "    payvals.pop()\n",
    "    payvals.pop()\n",
    "    for pay_count, pay_d in enumerate(payvals):\n",
    "        rawpvalso = []\n",
    "        rawpvals = []\n",
    "        rawpformat = {}\n",
    "        rawpvals.append(rawpformat)\n",
    "        rawpvalso.append(pay_d.text.replace(\" \", \"\"))\n",
    "        respval = rawpvalso[0].replace(\",\", \"\").replace(\"\\n\", \",\")\n",
    "        resphost = respval.split(\",\")\n",
    "        del resphost[0]\n",
    "        resphost.pop()\n",
    "        for p_count, p_v in enumerate(resphost):\n",
    "            rawpformat[paymentHeads[p_count]]=cleanSpecialCharAndSpace(p_v)\n",
    "        paymentVals.append(rawpformat)            \n",
    "    pform[paymentHeads[-11]+\"Gross\"]=cleanSpecialCharAndSpace(paymentHeads[-10])\n",
    "    pform['ActualPaymentCumulativeToDate(A)Net']=cleanSpecialCharAndSpace(paymentHeads[-9])\n",
    "    pform['ActualPaymentCumulativeToDate(A)Tax']=cleanSpecialCharAndSpace(paymentHeads[-8])\n",
    "    pform[paymentHeads[-6]+\"Gross\"]=cleanSpecialCharAndSpace(paymentHeads[-5])\n",
    "    hostpaytext = pay_soup.find('div', attrs={'id': 'c_payment'}).findAll('p')\n",
    "    if hostpaytext is not None:\n",
    "        pform['TaxDescription']= hostpaytext[1].text\n",
    "    paymentVals.append(pform)\n",
    "    contract_summary['payment']=paymentVals\n",
    "    \n",
    "    return contract_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brown-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1648 1649\n",
      "./scrapeddata/250714_TboungKhmum_TboungKhmum_TonleBet.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(1649):\n",
    "    pathname = \"\"\n",
    "    projects = []\n",
    "    finalformat = {}\n",
    "    print(i, len(host))\n",
    "    for j in host[i]['projects']:\n",
    "        wantedformat = {}\n",
    "        getlocation = \"\"\n",
    "        \n",
    "        try:\n",
    "            wantedformat['project_name']=j['project_name']\n",
    "        except KeyError:\n",
    "            wantedformat['project_name']=\"\"\n",
    "            \n",
    "        try:\n",
    "            pidval = splitPID(j['project_link'])\n",
    "            single_p_summary=getPHTML(j['project_html'], pidval)\n",
    "            wantedformat['project_link']=j['project_link']\n",
    "            wantedformat['project_html_data']=single_p_summary\n",
    "        except KeyError:\n",
    "            wantedformat['project_link']=\"\"\n",
    "            wantedformat['project_html_data']=[]\n",
    "          \n",
    "        try:\n",
    "            wantedformat['contract_id']=j['contract_id']\n",
    "        except KeyError:\n",
    "            wantedformat['contract_id']=\"\"\n",
    "            \n",
    "        try:\n",
    "            idval = splitPID(j['contract_link'])\n",
    "            single_c_summary=getCHTML(j['contract_html'], idval)\n",
    "            wantedformat['contract_link']=j['contract_link']\n",
    "            wantedformat['contract_html_data']=single_c_summary\n",
    "        except KeyError:\n",
    "            wantedformat['contract_link']=\"\"\n",
    "            wantedformat['contract_html_data']=[]\n",
    "\n",
    "        projects.append(wantedformat)\n",
    "        for plv in single_p_summary['ProjectSummary']:\n",
    "            getlocation=plv['Location']\n",
    "        pathname = getlocation.replace(\"[ \", \"\").replace(\" ] \", \"_\").replace(\" \\ \", \"_\").replace(\" \", \"\")+\".json\"\n",
    "    finalformat['location']=host[i]['location']\n",
    "    finalformat['projects']=projects\n",
    "    write_data(finalformat, pathname)\n",
    "#     print(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interpreted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     for j in host[i]['projects']:\n",
    "#         wantedformat = {}\n",
    "#         getlocation = \"\"\n",
    "#         pidval = splitPID(j['project_link'])\n",
    "#         idval = splitPID(j['contract_link'])\n",
    "#         single_p_summary=getPHTML(j['project_html'], pidval)\n",
    "#         single_c_summary=getCHTML(j['contract_html'], idval)\n",
    "#         print(j['contract_id'])\n",
    "#         wantedformat['project_name']=j['project_name']\n",
    "#         wantedformat['project_link']=j['project_link']\n",
    "#         wantedformat['contract_link']=j['contract_link']\n",
    "#         wantedformat['project_html_data']=single_p_summary\n",
    "#         wantedformat['contract_html_data']=single_c_summary\n",
    "#         for plv in single_p_summary['ProjectSummary']:\n",
    "#             getlocation=plv['Location']\n",
    "# #         print(getlocation.replace(\"[ \", \"\").replace(\" ] \", \"_\").replace(\" \\ \", \"_\").replace(\" \", \"\"))\n",
    "#         write_data(wantedformat, getlocation.replace(\"[ \", \"\").replace(\" ] \", \"_\").replace(\" \\ \", \"_\").replace(\" \", \"\")+\".json\")\n",
    "# #         print(\"\\n\")\n",
    "# #         print(single_p_summary)\n",
    "# #         print(\"\\n\")\n",
    "# #         hostList.append(single_p_summary)\n",
    "# #         chostList.append(single_c_summary)\n",
    "# #     write_data(hostList, \"project_detail_list.json\")\n",
    "# #     write_data(chostList, \"contract_detail_list.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "partial-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pidMatcher(val):\n",
    "#     for i in hostList:\n",
    "#         if len(i) != 0 and i['Pid'] is not None:\n",
    "#             if i['Pid'] == val:\n",
    "#                 return i\n",
    "#             else:\n",
    "#                 return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "numerical-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def idMatcher(val):\n",
    "#     for i in chostList:\n",
    "#         if len(i) != 0 and i['Id'] is not None:  \n",
    "#             if i['Id'] == val:\n",
    "#                 return i\n",
    "#             else:\n",
    "#                 return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collected-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     lhost = {}\n",
    "#     pjlist = []\n",
    "#     lhost['location']=linkinghost[i]['location']\n",
    "#     print(\"\\nLinkinHost\\n\")\n",
    "#     for j in linkinghost[i]['projects']:\n",
    "#         pidval = splitPID(j['project_link'])\n",
    "#         idval = splitPID(j['contract_link'])\n",
    "#         resp = pidMatcher(pidval)\n",
    "#         resc = idMatcher(idval)\n",
    "#         if resp is not None:\n",
    "#             j['project_data']=resp\n",
    "#         if resc is not None:\n",
    "#             j['contract_data']=resc\n",
    "#         pjlist.append(j)\n",
    "#         print(\"\\n\")\n",
    "#         print(j)\n",
    "#         print(\"\\n\")\n",
    "#     lhost['projects']=pjlist\n",
    "#     write_data(lhost, \"final_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-roots",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-sydney",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
